{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from create_flop_data import FlopDataset, flop_collate_fn\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import flop_to_vector, flop_look_up, vec_to_flop\n",
    "from utils import (\n",
    "    SUITEDNESS_DICT, \n",
    "    PAIRNESS_DICT,\n",
    "    CONNECTEDNESS_DICT,\n",
    "    HIGH_LOW_TEXTURE_DICT,\n",
    "    HIGH_CARD_DICT,\n",
    "    STRAIGHTNESS_DICT\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('flopdata.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flop</th>\n",
       "      <th>flop_encoded</th>\n",
       "      <th>suitedness</th>\n",
       "      <th>pairness</th>\n",
       "      <th>connectedness</th>\n",
       "      <th>high_low_texture</th>\n",
       "      <th>high_card</th>\n",
       "      <th>straightness</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2c, 2h, 2s]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2c</td>\n",
       "      <td>2h</td>\n",
       "      <td>2s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2d, 2h, 2s]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2d</td>\n",
       "      <td>2h</td>\n",
       "      <td>2s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2h, 2s, 3s]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2h</td>\n",
       "      <td>2s</td>\n",
       "      <td>3s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2h, 2s, 3h]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2h</td>\n",
       "      <td>2s</td>\n",
       "      <td>3h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2h, 2s, 3c]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2h</td>\n",
       "      <td>2s</td>\n",
       "      <td>3c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           flop                                       flop_encoded  \\\n",
       "0  [2c, 2h, 2s]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [2d, 2h, 2s]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [2h, 2s, 3s]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3  [2h, 2s, 3h]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "4  [2h, 2s, 3c]  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "\n",
       "   suitedness  pairness  connectedness  high_low_texture  high_card  \\\n",
       "0         0.0       1.0            0.0               0.0        0.0   \n",
       "1         0.0       1.0            0.0               0.0        0.0   \n",
       "2         1.0       1.0            1.0               0.0        1.0   \n",
       "3         1.0       1.0            1.0               0.0        1.0   \n",
       "4         0.0       1.0            1.0               0.0        1.0   \n",
       "\n",
       "   straightness card1 card2 card3  \n",
       "0           0.0    2c    2h    2s  \n",
       "1           0.0    2d    2h    2s  \n",
       "2           0.0    2h    2s    3s  \n",
       "3           0.0    2h    2s    3h  \n",
       "4           0.0    2h    2s    3c  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data = FlopDataset(data=df)\n",
    "trainloader = DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=flop_collate_fn)\n",
    "ex_batch = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 51])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "    def __init__(self, latent_dim = 6):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=51, out_features=40),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=40, out_features=30),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=30, out_features=20),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(20, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(20, latent_dim) \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=self.latent_dim, out_features=20),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=20, out_features=30),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=30, out_features=40),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=40, out_features=51),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar) \n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_log_var(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_recon, x, mu, log_var):\n",
    "    recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + kl_divergence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseVAE()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1204.5489\n",
      "Epoch [11/100], Loss: 1016.5307\n",
      "Epoch [21/100], Loss: 987.0770\n",
      "Epoch [31/100], Loss: 966.7626\n",
      "Epoch [41/100], Loss: 951.4088\n",
      "Epoch [51/100], Loss: 946.6270\n",
      "Epoch [61/100], Loss: 946.3825\n",
      "Epoch [71/100], Loss: 944.7838\n",
      "Epoch [81/100], Loss: 943.6098\n",
      "Epoch [91/100], Loss: 942.5611\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() \n",
    "    tot_loss = 0\n",
    "    for flop_vec, flop, suitedness,pairness, connectedness, high_low_texture, high_card, straightness in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x = flop_vec.to(torch.float32)\n",
    "        x_recon, mu, log_var = model(x) \n",
    "    \n",
    "        loss = loss_function(x_recon, x, mu, log_var)\n",
    "        tot_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = tot_loss / len(trainloader)\n",
    "    if (epoch % 10) == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flop_comp(model, num_flops = 1, device=\"cpu\"):\n",
    "    model.eval()  \n",
    "\n",
    "    z_sample = torch.randn(num_flops, model.latent_dim).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        flop_recon = model.decoder(z_sample)\n",
    "    return flop_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genflop_to_binary(flop_recon):\n",
    "    flop_binary = torch.zeros_like(flop_recon)\n",
    "\n",
    "    flop_binary[:, :13] = torch.nn.functional.one_hot(flop_recon[:, :13].argmax(dim=1), num_classes=13)\n",
    "    flop_binary[:, 13:17] = torch.nn.functional.one_hot(flop_recon[:, 13:17].argmax(dim=1), num_classes=4)\n",
    "    \n",
    "    flop_binary[:, 17:30] = torch.nn.functional.one_hot(flop_recon[:, 17:30].argmax(dim=1), num_classes=13)\n",
    "    flop_binary[:, 30:34] = torch.nn.functional.one_hot(flop_recon[:, 30:34].argmax(dim=1), num_classes=4)\n",
    "    \n",
    "    flop_binary[:, 34:47] = torch.nn.functional.one_hot(flop_recon[:, 34:47].argmax(dim=1), num_classes=13)\n",
    "    flop_binary[:, 47:] = torch.nn.functional.one_hot(flop_recon[:, 47:].argmax(dim=1), num_classes=4)\n",
    "\n",
    "    return flop_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_binary_flop(flop_batch):\n",
    "    RANKS = [str(i) for i in range(2, 10)] + ['T', 'J', 'Q', 'K', 'A']\n",
    "    SUITS = ['s', 'h', 'c', 'd']\n",
    "    \n",
    "    card1_ranks = np.argmax(flop_batch[:, :13], axis=1)\n",
    "    card1_suits = np.argmax(flop_batch[:, 13:17], axis=1)\n",
    "    \n",
    "    card2_ranks = np.argmax(flop_batch[:, 17:30], axis=1)\n",
    "    card2_suits = np.argmax(flop_batch[:, 30:34], axis=1)\n",
    "    \n",
    "    card3_ranks = np.argmax(flop_batch[:, 34:47], axis=1)\n",
    "    card3_suits = np.argmax(flop_batch[:, 47:], axis=1)\n",
    "    flops = [\n",
    "        [f\"{RANKS[card1_ranks[i]]}{SUITS[card1_suits[i]]}\",\n",
    "         f\"{RANKS[card2_ranks[i]]}{SUITS[card2_suits[i]]}\",\n",
    "         f\"{RANKS[card3_ranks[i]]}{SUITS[card3_suits[i]]}\"]\n",
    "        for i in range(flop_batch.shape[0])\n",
    "    ]\n",
    "    return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flop_human(model, num_flops = 1, device=\"cpu\"):\n",
    "    flop_recon = generate_flop_comp(model, num_flops, device)\n",
    "    flop_bin = genflop_to_binary(flop_recon)\n",
    "    decoded_flops = decode_binary_flop(flop_bin)\n",
    "    return decoded_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3d', '3s', '7h'],\n",
       " ['9d', 'Kd', 'Qs'],\n",
       " ['Js', 'Tc', 'Ts'],\n",
       " ['9d', 'Kd', 'Qs'],\n",
       " ['2d', '8d', 'Th'],\n",
       " ['2d', '9d', 'Ah'],\n",
       " ['8d', '9d', 'Ah'],\n",
       " ['4d', '5d', '7h'],\n",
       " ['9d', 'Ad', 'Ts'],\n",
       " ['4s', 'Qc', 'Ts']]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_flop_human(model, num_flops=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
